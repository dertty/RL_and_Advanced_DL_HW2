{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# ENV"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "import tqdm\n",
    "\n",
    "N_ROWS, N_COLS, N_WIN = 3, 3, 3\n",
    "\n",
    "class TicTacToe(gym.Env):\n",
    "    def __init__(self, n_rows=N_ROWS, n_cols=N_COLS, n_win=N_WIN):\n",
    "        self.n_rows = n_rows\n",
    "        self.n_cols = n_cols\n",
    "        self.n_win = n_win\n",
    "\n",
    "        self.board = np.zeros((self.n_rows, self.n_cols), dtype=int)\n",
    "        self.gameOver = False\n",
    "        self.boardHash = None\n",
    "        # ход первого игрока\n",
    "        self.curTurn = 1\n",
    "        self.emptySpaces = None\n",
    "\n",
    "        self.reset()\n",
    "\n",
    "    def getEmptySpaces(self):\n",
    "        if self.emptySpaces is None:\n",
    "            res = np.where(self.board == 0)\n",
    "            self.emptySpaces = np.array([ (i, j) for i,j in zip(res[0], res[1]) ])\n",
    "        return self.emptySpaces\n",
    "\n",
    "    def makeMove(self, player, i, j):\n",
    "        self.board[i, j] = player\n",
    "        self.emptySpaces = None\n",
    "        self.boardHash = None\n",
    "\n",
    "    def getHash(self):\n",
    "        if self.boardHash is None:\n",
    "            self.boardHash = ''.join(['%s' % (x+1) for x in self.board.reshape(self.n_rows * self.n_cols)])\n",
    "        return self.boardHash\n",
    "\n",
    "    def _check_terminal(self, cur_p):\n",
    "        cur_marks = np.where(self.board == cur_p)\n",
    "        for i,j in zip(cur_marks[0], cur_marks[1]):\n",
    "            if i <= self.n_rows - self.n_win:\n",
    "                if np.all(self.board[i:i+self.n_win, j] == cur_p):\n",
    "                    return True\n",
    "            if j <= self.n_cols - self.n_win:\n",
    "                if np.all(self.board[i,j:j+self.n_win] == cur_p):\n",
    "                    return True\n",
    "            if i <= self.n_rows - self.n_win and j <= self.n_cols - self.n_win:\n",
    "                if np.all(np.array([ self.board[i+k,j+k] == cur_p for k in range(self.n_win) ])):\n",
    "                    return True\n",
    "            if i <= self.n_rows - self.n_win and j >= self.n_win-1:\n",
    "                if np.all(np.array([ self.board[i+k,j-k] == cur_p for k in range(self.n_win) ])):\n",
    "                    return True\n",
    "        return False\n",
    "\n",
    "    def isTerminal(self):\n",
    "        # проверим, не закончилась ли игра\n",
    "        cur_win = self._check_terminal(self.curTurn)\n",
    "        if cur_win:\n",
    "                self.gameOver = True\n",
    "                return self.curTurn\n",
    "\n",
    "        if len(self.getEmptySpaces()) == 0:\n",
    "            self.gameOver = True\n",
    "            return 0\n",
    "\n",
    "        self.gameOver = False\n",
    "        return None\n",
    "\n",
    "    def getWinner(self):\n",
    "        # фактически запускаем isTerminal два раза для крестиков и ноликов\n",
    "        if self._check_terminal(1):\n",
    "            return 1\n",
    "        if self._check_terminal(-1):\n",
    "            return -1\n",
    "        if len(self.getEmptySpaces()) == 0:\n",
    "            return 0\n",
    "        return None\n",
    "\n",
    "    def printBoard(self):\n",
    "        for i in range(0, self.n_rows):\n",
    "            print('----'*(self.n_cols)+'-')\n",
    "            out = '| '\n",
    "            for j in range(0, self.n_cols):\n",
    "                if self.board[i, j] == 1:\n",
    "                    token = 'x'\n",
    "                if self.board[i, j] == -1:\n",
    "                    token = 'o'\n",
    "                if self.board[i, j] == 0:\n",
    "                    token = ' '\n",
    "                out += token + ' | '\n",
    "            print(out)\n",
    "        print('----'*(self.n_cols)+'-')\n",
    "\n",
    "    def getState(self):\n",
    "        return (self.getHash(), self.getEmptySpaces(), self.curTurn)\n",
    "\n",
    "    def action_from_int(self, action_int):\n",
    "        return ( int(action_int / self.n_cols), int(action_int % self.n_cols))\n",
    "\n",
    "    def int_from_action(self, action):\n",
    "        return action[0] * self.n_cols + action[1]\n",
    "\n",
    "    def step(self, action):\n",
    "        if self.board[action[0], action[1]] != 0:\n",
    "            return self.getState(), -10, True, {}\n",
    "        self.makeMove(self.curTurn, action[0], action[1])\n",
    "        reward = self.isTerminal()\n",
    "        self.curTurn = -self.curTurn\n",
    "        return self.getState(), 0 if reward is None else reward, reward is not None, {}\n",
    "\n",
    "    def reset(self):\n",
    "        self.board = np.zeros((self.n_rows, self.n_cols), dtype=int)\n",
    "        self.boardHash = None\n",
    "        self.gameOver = False\n",
    "        self.emptySpaces = None\n",
    "        self.curTurn = 1"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    " # RL and Advanced DL: Домашнее задание 2\n",
    "## Часть первая: крестики-нолики при помощи Q-обучения\n",
    "### Реализуйте обычное (табличное) Q-обучение. Обучите стратегии крестиков и ноликов для доски 3х3."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "class Q_learning:\n",
    "\n",
    "    def __init__(self, n_rows=3, n_cols=3, n_win=3, lr=0.01, gamma=1):\n",
    "        self.lr = lr\n",
    "        self.gamma = gamma\n",
    "        self.env = TicTacToe()\n",
    "\n",
    "    @staticmethod\n",
    "    def get_possible_actions():\n",
    "        return [env.int_from_action(env.getEmptySpaces()[i]) for i in range(len(env.getEmptySpaces()))]\n",
    "\n",
    "    def greedy_strategy(self, env, Q, epsilon = 0.01):\n",
    "        possible_actions = self.get_possible_actions()\n",
    "        if env.getState()[0] not in Q.keys():\n",
    "            Q[env.getState()[0]] = np.zeros(len(self.get_possible_actions()))\n",
    "\n",
    "        if np.random.random() > epsilon:\n",
    "            return np.argmax(Q[env.getState()[0]]), self.get_possible_actions()[np.argmax(Q[env.getState()[0]])]\n",
    "        else:\n",
    "            ch = np.random.choice(len(possible_actions))\n",
    "            return ch, possible_actions[ch]\n",
    "\n",
    "\n",
    "    def game(self, env, Q, crosses=1):\n",
    "        env.reset()\n",
    "\n",
    "        opponent_action = lambda: np.random.choice(self.get_possible_actions())\n",
    "\n",
    "        if env.curTurn != crosses:\n",
    "            env.step(env.action_from_int(opponent_action()))\n",
    "\n",
    "        while True:\n",
    "            state = env.getState()[0]\n",
    "            q_action, action = self.greedy_strategy(env, Q)\n",
    "            _, reward, done, _ = env.step(env.action_from_int(action))\n",
    "            reward = crosses * reward\n",
    "\n",
    "            if done:\n",
    "                Q[state][q_action] += self.lr * (reward  - Q[state][q_action])\n",
    "                return reward\n",
    "            else:\n",
    "                next_state_, reward, done, _ = env.step(env.action_from_int(opponent_action()))\n",
    "                reward = crosses * reward\n",
    "                if done:\n",
    "                    Q[state][q_action] += self.lr * (reward  - Q[state][q_action])\n",
    "                    return reward\n",
    "                else:\n",
    "                    next_state = next_state_[0]\n",
    "                    if next_state in Q.keys():\n",
    "                        Q[state][q_action] += self.lr * (reward + self.gamma * np.max(Q[next_state]) - Q[state][q_action])\n",
    "                    else:\n",
    "                        Q[next_state] = np.zeros(len(self.get_possible_actions()))\n",
    "                        Q[state][q_action] += self.lr * (reward + self.gamma * np.max(Q[next_state]) - Q[state][q_action])\n",
    "\n",
    "\n",
    "    def games(self, n_games=100000, crosses=1):\n",
    "        Q = {}\n",
    "        for i in tqdm.tqdm(range(n_games)):\n",
    "            self.game(self.env, Q, crosses=crosses)\n",
    "        return Q"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 3 x 3 x 3"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100000/100000 [00:18<00:00, 5533.39it/s]\n",
      "100%|██████████| 100000/100000 [00:12<00:00, 8104.63it/s]\n",
      "100%|██████████| 100000/100000 [00:12<00:00, 8290.97it/s]\n"
     ]
    }
   ],
   "source": [
    "games = Q_learning(n_rows=3, n_cols=3, n_win=3)\n",
    "Q_crosses = games.games()\n",
    "Q_nills = games.games(crosses = -1)\n",
    "Q_nills = games.games(crosses = -1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Попробуйте обучить стратегии крестиков и ноликов для доски 4х4 и/или 5х5."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 4 x 4 x 4"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100000/100000 [00:18<00:00, 5358.79it/s]\n",
      "100%|██████████| 100000/100000 [00:12<00:00, 8181.45it/s]\n",
      "100%|██████████| 100000/100000 [00:12<00:00, 7991.32it/s]\n"
     ]
    }
   ],
   "source": [
    "games = Q_learning(n_rows=4, n_cols=4, n_win=4)\n",
    "Q_crosses = games.games()\n",
    "Q_nills = games.games(crosses = -1)\n",
    "Q_nills = games.games(crosses = -1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 5 x 5 x 5"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100000/100000 [00:18<00:00, 5287.03it/s]\n",
      "100%|██████████| 100000/100000 [00:12<00:00, 7801.11it/s]\n",
      "100%|██████████| 100000/100000 [00:12<00:00, 7743.91it/s]\n"
     ]
    }
   ],
   "source": [
    "games = Q_learning(n_rows=5, n_cols=5, n_win=5)\n",
    "Q_crosses = games.games()\n",
    "Q_nills = games.games(crosses = -1)\n",
    "Q_nills = games.games(crosses = -1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Часть вторая: добавим нейронных сетей"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Часть третья: расширим и углубим поиск"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Часть четвёртая, опциональная: AlphaZero"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
